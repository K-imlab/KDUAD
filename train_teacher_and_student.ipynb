{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd69ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoader import DataLoader, CustomDataset, load_raw\n",
    "from model import AE\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0575e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(recon, origin):\n",
    "    n_feature = origin.shape[-1]\n",
    "\n",
    "    # calculate rmse\n",
    "    error = tf.math.subtract(recon, origin)\n",
    "    error = tf.math.pow(error, 2)\n",
    "    error = tf.math.reduce_sum(error, axis=1)\n",
    "    error = tf.math.divide(error, n_feature)\n",
    "    error = tf.math.sqrt(error)\n",
    "    # calculate mean of rmse value by batch for train\n",
    "    error_mean = tf.reduce_mean(error)\n",
    "\n",
    "    # calculate median and maximum of rmse value by batch for test\n",
    "    error_array = np.array(error)\n",
    "    error_median = np.median(error_array)\n",
    "    error_maximum = np.max(error_array)\n",
    "    error_minimum = np.min(error_array)\n",
    "    return error_mean, error_median, error_maximum, error_minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_usad(train_x, epoch):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        w1, w2, w3 = teacher_AE(train_x)\n",
    "        loss1 = usad_loss(step=1, recon=w1, rerecon=w3, origin=train_x, n=epoch + 1)\n",
    "\n",
    "    gradients_enc = tape.gradient(loss1, teacher_AE.encoder.trainable_variables)\n",
    "    gradients_dec = tape.gradient(loss1, teacher_AE.decoder.trainable_variables)\n",
    "    OPTIMIZER.apply_gradients(zip(gradients_enc, teacher_AE.encoder.trainable_variables))\n",
    "    OPTIMIZER.apply_gradients(zip(gradients_dec, teacher_AE.decoder.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        w1, w2, w3 = teacher_AE(train_x)\n",
    "        loss2 = usad_loss(step=2, recon=w2, rerecon=w3, origin=train_x, n=epoch + 1)\n",
    "\n",
    "    gradients_enc = tape.gradient(loss2, teacher_AE.encoder.trainable_variables)\n",
    "    gradients_dec = tape.gradient(loss2, teacher_AE.decoder2.trainable_variables)\n",
    "    OPTIMIZER.apply_gradients(zip(gradients_enc, teacher_AE.encoder.trainable_variables))\n",
    "    OPTIMIZER.apply_gradients(zip(gradients_dec, teacher_AE.decoder2.trainable_variables))\n",
    "\n",
    "    return loss1, loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7676487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step_usad(val_x, epoch) :\n",
    "    w1, w2, w3 = teacher_AE(val_x)\n",
    "\n",
    "    loss1 = usad_loss(1, w1, w3, val_x, epoch+1)\n",
    "    loss2 = usad_loss(2, w2, w3, val_x, epoch+1)\n",
    "\n",
    "    return loss1, loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def usad_loss(step, recon, rerecon, origin, n=1, a=1) :\n",
    "    loss1, loss1_median, loss1_max, loss1_min = rmse_loss(recon, origin)\n",
    "    loss2, loss2_median, loss2_max, loss2_min = rmse_loss(rerecon, origin)\n",
    "\n",
    "    # Step teacher : Train\n",
    "    # Step 2 : Validation\n",
    "    # Step 3 : Test\n",
    "    if step == 1:\n",
    "        loss = tf.abs(((1/n) * loss1) + ((1-(1/n))*loss2))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    elif step == 2:\n",
    "        loss = tf.abs(((1/n) * loss1) - ((1-(1/n))*loss2))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    elif step == 3:\n",
    "        mean = (a * loss1) + ((1-a) * loss2)\n",
    "        median = (a * loss1_median) + ((1-a) * loss2_median)\n",
    "        max = (a * loss1_max) + ((1-a) * loss2_max)\n",
    "        min = (a * loss1_min) + ((1-a) * loss2_min)\n",
    "\n",
    "        return mean.numpy(), median, max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d97e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"result/teacher\"\n",
    "teacher_n_input = 52\n",
    "student_n_input = 18\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "\n",
    "teacher_n_features = [teacher_n_input, 256, 128, 64, 32, 18]\n",
    "student_n_features = [student_n_input, 256, 128, 64, 32, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f07f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_AE = AE(teacher_n_features)\n",
    "student_AE = AE(student_n_features)\n",
    "OPTIMIZER = Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y, test, additional_test = load_raw()\n",
    "\n",
    "train_set = CustomDataset(train_X, train_y)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_set = CustomDataset(val_X, val_y)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_X = CustomDataset(test, None)\n",
    "test_loader = DataLoader(test_X, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "label_test = CustomDataset(additional_test, pd.DataFrame(np.ones_like(additional_test)))\n",
    "label_test_loader = DataLoader(label_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95fcd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'train_loss': [], 'val_loss': [], 'train_loss1': [],\n",
    "           'train_loss2': [], 'val_loss1': [], 'val_loss2': []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    train_loss2 = []\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        teacher_train_data, label = data\n",
    "        loss1, loss2 = train_step_usad(teacher_train_data, epoch)\n",
    "        print(\"\\rTraining : {} / {} \".format(i + 1, len(train_loader)), end=\"\")\n",
    "        train_loss.append(loss1)\n",
    "        train_loss2.append(loss2)\n",
    "\n",
    "    train_loader.on_epoch_end()\n",
    "    print(\"\\tTraining is completed...\")\n",
    "\n",
    "    val_loss = []\n",
    "    val_loss2 = []\n",
    "    for j, data in enumerate(val_loader):\n",
    "        teacher_val_data, label = data\n",
    "        loss1, loss2 = val_step_usad(teacher_val_data, epoch)\n",
    "        print(\"\\rValidation : {} / {}\".format(j + 1, len(val_loader)), end=\"\")\n",
    "        val_loss.append(loss1)\n",
    "        val_loss2.append(loss2)\n",
    "\n",
    "    val_loader.on_epoch_end()\n",
    "    print(\"\\tValidation is completed...\")\n",
    "\n",
    "    train_loss_avg = sum(train_loss) / len(train_loss)\n",
    "    val_loss_avg = sum(val_loss) / len(val_loss)\n",
    "    train_loss2_avg = sum(train_loss2) / len(train_loss2)\n",
    "    val_loss2_avg = sum(val_loss2) / len(val_loss2)\n",
    "    results['train_loss1'].append(train_loss_avg.numpy())\n",
    "    results['train_loss2'].append(train_loss2_avg.numpy())\n",
    "    results['val_loss1'].append(val_loss_avg.numpy())\n",
    "    results['val_loss2'].append(val_loss2_avg.numpy())\n",
    "\n",
    "    teacher_AE.save_weights(os.path.join(save_path, f\"{epoch + 1: 05d} epoch_weights\"))\n",
    "\n",
    "    if epoch > 0:\n",
    "        if (val_loss_avg + val_loss2_avg) / 2 < min(results['val_loss']):\n",
    "            teacher_AE.save_weights(os.path.join(save_path, \"Best_weights\"))\n",
    "\n",
    "        if val_loss_avg.numpy() < min(results['val_loss']):\n",
    "            teacher_AE.save_weights(os.path.join(save_path, \"Best_weights\"))\n",
    "\n",
    "    results['train_loss'].append(((train_loss_avg + train_loss2_avg) / 2).numpy())\n",
    "    results['val_loss'].append(((val_loss_avg + val_loss2_avg) / 2).numpy())\n",
    "\n",
    "    print(\n",
    "        \"{:>3} / {:>3} || train_loss: {:8.4f}, val_loss: {:8.4f}\".format(\n",
    "            epoch + 1, epochs,\n",
    "            results['train_loss'][-1],\n",
    "            results['val_loss'][-1], ))\n",
    "    print(\"_\"*30)\n",
    "    # early stop\n",
    "    if epoch > 40:\n",
    "        if results['val_loss'][-11] < min(results['val_loss'][-10:]):\n",
    "            print(results['val_loss'][-11])\n",
    "            print(min(results['val_loss'][-11:]))\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(save_path, 'train_results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9\n",
    "for x, label in label_test_loader:\n",
    "    w1, w2, w3 = teacher_AE.predict(x)\n",
    "    mean, median, maximum, minimum = usad_loss(step=3, recon=w1, rerecon=w3, origin=x, a=alpha)\n",
    "\n",
    "    print(mean.shape, label.shape)\n",
    "\n",
    "    opt_threshold = utils.draw_roc(label, mean)\n",
    "    accuracy, f1, recall, precision = utils.get_metric(label, mean, opt_threshold)\n",
    "    print(accuracy, f1, recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "421dc470",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36120253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoader import DataLoader, CustomDataset, load_raw\n",
    "from model import AE\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(recon, origin):\n",
    "    n_feature = origin.shape[-1]\n",
    "\n",
    "    # calculate rmse\n",
    "    error = tf.math.subtract(recon, origin)\n",
    "    error = tf.math.pow(error, 2)\n",
    "    error = tf.math.reduce_sum(error, axis=1)\n",
    "    error = tf.math.divide(error, n_feature)\n",
    "    error = tf.math.sqrt(error)\n",
    "    # calculate mean of rmse value by batch for train\n",
    "    error_mean = tf.reduce_mean(error)\n",
    "\n",
    "    # calculate median and maximum of rmse value by batch for test\n",
    "    error_array = np.array(error)\n",
    "    error_median = np.median(error_array)\n",
    "    error_maximum = np.max(error_array)\n",
    "    error_minimum = np.min(error_array)\n",
    "    return error_mean, error_median, error_maximum, error_minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31418d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_distill(teacher_train_x, student_train_x, epoch):\n",
    "    origin1, origin2, _ = teacher_AE.predict(teacher_train_x, verbose=0)\n",
    "    origin1 = origin1[:, test_cols]\n",
    "    origin2 = origin2[:, test_cols]\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        w1, w2, w3 = student_AE(student_train_x)\n",
    "        loss1 = usad_loss(step=1, recon=w1, rerecon=w3, origin=origin1, n=epoch + 1)\n",
    "\n",
    "    gradients_enc = tape.gradient(loss1, student_AE.encoder.trainable_variables)\n",
    "    gradients_dec = tape.gradient(loss1, student_AE.decoder.trainable_variables)\n",
    "    OPTIMIZER.apply_gradients(zip(gradients_enc, student_AE.encoder.trainable_variables))\n",
    "    OPTIMIZER.apply_gradients(zip(gradients_dec, student_AE.decoder.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        w1, w2, w3 = student_AE(student_train_x)\n",
    "        loss2 = usad_loss(step=2, recon=w2, rerecon=w3, origin=origin2, n=epoch + 1)\n",
    "\n",
    "    gradients_enc = tape.gradient(loss2, student_AE.encoder.trainable_variables)\n",
    "    gradients_dec = tape.gradient(loss2, student_AE.decoder2.trainable_variables)\n",
    "    OPTIMIZER.apply_gradients(zip(gradients_enc, student_AE.encoder.trainable_variables))\n",
    "    OPTIMIZER.apply_gradients(zip(gradients_dec, student_AE.decoder2.trainable_variables))\n",
    "\n",
    "    return loss1, loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8154694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step_distill(teacher_val_x, student_val_x, epoch):\n",
    "    origin1, origin2, _ = teacher_AE.predict(teacher_val_x, verbose=0)\n",
    "    origin1 = origin1[:, test_cols]\n",
    "    origin2 = origin2[:, test_cols]\n",
    "    w1, w2, w3 = student_AE(student_val_x)\n",
    "\n",
    "    loss1 = usad_loss(1, w1, w3, origin1, epoch+1)\n",
    "    loss2 = usad_loss(2, w2, w3, origin2, epoch+1)\n",
    "\n",
    "    return loss1, loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e1eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def usad_loss(step, recon, rerecon, origin, n=1, a=1):\n",
    "    loss1, loss1_median, loss1_max, loss1_min = rmse_loss(recon, origin)\n",
    "    loss2, loss2_median, loss2_max, loss2_min = rmse_loss(rerecon, origin)\n",
    "\n",
    "    # Step teacher : Train\n",
    "    # Step 2 : Validation\n",
    "    # Step 3 : Test\n",
    "    if step == 1:\n",
    "        loss = tf.abs(((1/n) * loss1) + ((1-(1/n))*loss2))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    elif step == 2:\n",
    "        loss = tf.abs(((1/n) * loss1) - ((1-(1/n))*loss2))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    elif step == 3:\n",
    "        mean = (a * loss1) + ((1-a) * loss2)\n",
    "        median = (a * loss1_median) + ((1-a) * loss2_median)\n",
    "        max = (a * loss1_max) + ((1-a) * loss2_max)\n",
    "        min = (a * loss1_min) + ((1-a) * loss2_min)\n",
    "\n",
    "        return mean.numpy(), median, max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c605fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_path = os.path.join(\"result\", \"teacher\")\n",
    "save_path = os.path.join(\"result\", \"student\")\n",
    "teacher_n_input = 52\n",
    "student_n_input = 18\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "\n",
    "teacher_n_features = [teacher_n_input, 256, 128, 64, 32, 18]\n",
    "student_n_features = [student_n_input, 256, 128, 64, 32, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ffa0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_AE = AE(teacher_n_features)\n",
    "student_AE = AE(student_n_features)\n",
    "\n",
    "OPTIMIZER = Adam(learning_rate=lr)\n",
    "\n",
    "teacher_AE.load_weights(os.path.join(teacher_path, \"Best_weights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ba9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y, test, additional_test = load_raw()\n",
    "\n",
    "train_set = CustomDataset(train_X, train_y, distillation=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_cols = train_set.data_X.columns.isin(train_set.test_stage_features)\n",
    "\n",
    "val_set = CustomDataset(val_X, val_y, distillation=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_X = CustomDataset(test, None, distillation=True)\n",
    "test_loader = DataLoader(test_X, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'train_loss': [], 'val_loss': [], 'train_loss1': [],\n",
    "           'train_loss2': [], 'val_loss1': [], 'val_loss2': []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    train_loss2 = []\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        teacher_train_data, student_train_data, label = data\n",
    "        loss1, loss2 = train_step_distill(teacher_train_data, student_train_data, epoch)\n",
    "        print(\"\\rTraining : {} / {} \".format(i + 1, len(train_loader)), end=\"\")\n",
    "        train_loss.append(loss1)\n",
    "        train_loss2.append(loss2)\n",
    "\n",
    "    train_loader.on_epoch_end()\n",
    "    print(\"\\tTraining is completed...\")\n",
    "\n",
    "    val_loss = []\n",
    "    val_loss2 = []\n",
    "    for j, data in enumerate(val_loader):\n",
    "        teacher_val_data, student_val_data, label = data\n",
    "        loss1, loss2 = val_step_distill(teacher_val_data, student_val_data, epoch)\n",
    "        print(\"\\rValidation : {} / {}\".format(j + 1, len(val_loader)), end=\"\")\n",
    "        val_loss.append(loss1)\n",
    "        val_loss2.append(loss2)\n",
    "\n",
    "    val_loader.on_epoch_end()\n",
    "    print(\"\\tValidation is completed...\")\n",
    "\n",
    "    train_loss_avg = sum(train_loss) / len(train_loss)\n",
    "    val_loss_avg = sum(val_loss) / len(val_loss)\n",
    "    train_loss2_avg = sum(train_loss2) / len(train_loss2)\n",
    "    val_loss2_avg = sum(val_loss2) / len(val_loss2)\n",
    "    results['train_loss1'].append(train_loss_avg.numpy())\n",
    "    results['train_loss2'].append(train_loss2_avg.numpy())\n",
    "    results['val_loss1'].append(val_loss_avg.numpy())\n",
    "    results['val_loss2'].append(val_loss2_avg.numpy())\n",
    "\n",
    "    teacher_AE.save_weights(os.path.join(save_path, f\"{epoch + 1: 05d} epoch_weights\"))\n",
    "\n",
    "    if epoch > 0:\n",
    "        if (val_loss_avg + val_loss2_avg) / 2 < min(results['val_loss']):\n",
    "            teacher_AE.save_weights(os.path.join(save_path, \"Best_weights\"))\n",
    "\n",
    "        if val_loss_avg.numpy() < min(results['val_loss']):\n",
    "            teacher_AE.save_weights(os.path.join(save_path, \"Best_weights\"))\n",
    "\n",
    "    results['train_loss'].append(((train_loss_avg + train_loss2_avg) / 2).numpy())\n",
    "    results['val_loss'].append(((val_loss_avg + val_loss2_avg) / 2).numpy())\n",
    "\n",
    "    print(\n",
    "        \"{:>3} / {:>3} || train_loss: {:8.4f}, val_loss: {:8.4f}\".format(\n",
    "            epoch + 1, epochs,\n",
    "            results['train_loss'][-1],\n",
    "            results['val_loss'][-1], ))\n",
    "    print(\"_\"*30)\n",
    "    # early stop\n",
    "    if epoch > 20:\n",
    "        if results['val_loss'][-5] < min(results['val_loss'][-4:]):\n",
    "\n",
    "            print(results['val_loss'][-5])\n",
    "            print(min(results['val_loss'][-4:]))\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(save_path, 'train_results.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
